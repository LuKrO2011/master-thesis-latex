%%%%%%%%%%%%%%%%%
% From Template %
%%%%%%%%%%%%%%%%%

\documentclass[%
class=scrreprt,
chapterprefix=false,%
open=right,%
twoside=false,%
paper=a4,%
logofile={Logo\_zentral\_farbig\_EN.png},%
thesistype=masterproposal,%
UKenglish,%
]{se2thesis}
\listfiles
\usepackage[ngerman,main=UKenglish]{babel}
\usepackage{blindtext}
\usepackage[%
csquotes=true,%
booktabs=true,%
siunitx=true,%
minted=true,%
selnolig=true,%
widowcontrol=false,%
microtype=true,%
% biblatex=true,%
cleveref=true,%
]{se2packages}

% Own change (from Lisa)
% Changing citeauthor to use et el.
\usepackage[maxcitenames=2,backend=biber]{biblatex}
% End own change

\begin{filecontents}{\jobname.bib}
	@article{linaker2015guidelines,
		author = {Lin√•ker, Johan and Sulaman, Sardar and Host, Martin and de Mello, Rafael},
		year = {2015},
		month = {05},
		pages = {},
		title = {Guidelines for Conducting Surveys in Software Engineering}
	}
	
	@article{boehm2001defect,
		title={Defect reduction top 10 list},
		author={Boehm, Barry and Basili, Victor R},
		journal={Computer},
		volume={34},
		number={1},
		pages={135--137},
		year={2001}
	}
	
	@article{buse2009learning,
		title={Learning a metric for code readability},
		author={Buse, Raymond PL and Weimer, Westley R},
		journal={IEEE Transactions on software engineering},
		volume={36},
		number={4},
		pages={546--558},
		year={2009},
		publisher={IEEE}
	}
	
	@inproceedings{aggarwal2002integrated,
		title={An integrated measure of software maintainability},
		author={Aggarwal, Krishan K and Singh, Yogesh and Chhabra, Jitender Kumar},
		booktitle={Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No. 02CH37318)},
		pages={235--241},
		year={2002},
		organization={IEEE}
	}
	
	@inproceedings{fakhoury2019improving,
		title={Improving source code readability: Theory and practice},
		author={Fakhoury, Sarah and Roy, Devjeet and Hassan, Adnan and Arnaoudova, Vernera},
		booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
		pages={2--12},
		year={2019},
		organization={IEEE}
	}
	
	@inproceedings{scalabrino2016improving,
		title={Improving code readability models with textual features},
		author={Scalabrino, Simone and Linares-Vasquez, Mario and Poshyvanyk, Denys and Oliveto, Rocco},
		booktitle={2016 IEEE 24th International Conference on Program Comprehension (ICPC)},
		pages={1--10},
		year={2016},
		organization={IEEE}
	}
	
	@inproceedings{posnett2011simpler,
		title={A simpler model of software readability},
		author={Posnett, Daryl and Hindle, Abram and Devanbu, Premkumar},
		booktitle={Proceedings of the 8th working conference on mining software repositories},
		pages={73--82},
		year={2011}
	}
	
	@article{dorn2012general,
		title={A general software readability model},
		author={Dorn, Jonathan},
		journal={MCS Thesis available from (http://www. cs. virginia. edu/weimer/students/dorn-mcs-paper. pdf)},
		volume={5},
		pages={11--14},
		year={2012}
	}
	
	@inproceedings{scalabrino2017automatically,
		title={Automatically assessing code understandability: How far are we?},
		author={Scalabrino, Simone and Bavota, Gabriele and Vendome, Christopher and Linares-V{\'a}squez, Mario and Poshyvanyk, Denys and Oliveto, Rocco},
		booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
		pages={417--427},
		year={2017},
		organization={IEEE}
	}
	
	@inproceedings{daka2015modeling,
		title={Modeling readability to improve unit tests},
		author={Daka, Ermira and Campos, Jos{\'e} and Fraser, Gordon and Dorn, Jonathan and Weimer, Westley},
		booktitle={Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
		pages={107--118},
		year={2015}
	}
	
	@article{mi2022towards,
		title={Towards using visual, semantic and structural features to improve code readability classification},
		author={Mi, Qing and Hao, Yiqun and Ou, Liwei and Ma, Wei},
		journal={Journal of Systems and Software},
		volume={193},
		pages={111454},
		year={2022},
		publisher={Elsevier}
	}
	
	@inproceedings{mi2018gamification,
		title={A gamification technique for motivating students to learn code readability in software engineering},
		author={Mi, Qing and Keung, Jacky and Mei, Xiupei and Xiao, Yan and Chan, WK},
		booktitle={2018 International Symposium on Educational Technology (ISET)},
		pages={250--254},
		year={2018},
		organization={IEEE}
	}
	
	@article{mi2018improving,
		title={Improving code readability classification using convolutional neural networks},
		author={Mi, Qing and Keung, Jacky and Xiao, Yan and Mensah, Solomon and Gao, Yujin},
		journal={Information and Software Technology},
		volume={104},
		pages={60--71},
		year={2018},
		publisher={Elsevier}
	}
	
	@book{brooks1987no,
		title={No silver bullet},
		author={Brooks, Frederick and Kugler, H},
		year={1987},
		publisher={April}
	}
	
	@article{loriot2022styler,
		title={Styler: learning formatting conventions to repair Checkstyle violations},
		author={Loriot, Benjamin and Madeiral, Fernanda and Monperrus, Martin},
		journal={Empirical Software Engineering},
		volume={27},
		number={6},
		pages={149},
		year={2022},
		publisher={Springer}
	}
	
	@inproceedings{yasunaga2020graph,
		title={Graph-based, self-supervised program repair from diagnostic feedback},
		author={Yasunaga, Michihiro and Liang, Percy},
		booktitle={International Conference on Machine Learning},
		pages={10799--10808},
		year={2020},
		organization={PMLR}
	}
	
	@inproceedings{xu2019method,
		title={Method name suggestion with hierarchical attention networks},
		author={Xu, Sihan and Zhang, Sen and Wang, Weijing and Cao, Xinya and Guo, Chenkai and Xu, Jing},
		booktitle={Proceedings of the 2019 ACM SIGPLAN workshop on partial evaluation and program manipulation},
		pages={10--21},
		year={2019}
	}
	
	@inproceedings{allamanis2016convolutional,
		title={A convolutional attention network for extreme summarization of source code},
		author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
		booktitle={International conference on machine learning},
		pages={2091--2100},
		year={2016},
		organization={PMLR}
	}
	
	@article{hestness2017deep,
		title={Deep learning scaling is predictable, empirically},
		author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
		journal={arXiv preprint arXiv:1712.00409},
		year={2017}
	}
	
	@inproceedings{liu2019learning,
		title={Learning to spot and refactor inconsistent method names},
		author={Liu, Kui and Kim, Dongsun and Bissyand{\'e}, Tegawend{\'e} F and Kim, Taeyoung and Kim, Kisub and Koyuncu, Anil and Kim, Suntae and Le Traon, Yves},
		booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
		pages={1--12},
		year={2019},
		organization={IEEE}
	}
	
	@article{deimel1985uses,
		title={The uses of program reading},
		author={Deimel Jr, Lionel E},
		journal={ACM SIGCSE Bulletin},
		volume={17},
		number={2},
		pages={5--14},
		year={1985},
		publisher={ACM New York, NY, USA}
	}
	
	@inproceedings{raymond1991reading,
		title={Reading source code.},
		author={Raymond, Darrell R},
		booktitle={CASCON},
		volume={91},
		pages={3--16},
		year={1991}
	}
	
	@article{rugaber2000use,
		title={The use of domain knowledge in program understanding},
		author={Rugaber, Spencer},
		journal={Annals of Software Engineering},
		volume={9},
		number={1-4},
		pages={143--192},
		year={2000},
		publisher={Springer}
	}
	
	@article{likert1932technique,
		title={A technique for the measurement of attitudes.},
		author={Likert, Rensis},
		journal={Archives of psychology},
		year={1932}
	}
	
	@inproceedings{wyrich2019towards,
		title={Towards an autonomous bot for automatic source code refactoring},
		author={Wyrich, Marvin and Bogner, Justus},
		booktitle={2019 IEEE/ACM 1st international workshop on bots in software engineering (BotSE)},
		pages={24--28},
		year={2019},
		organization={IEEE}
	}
	
	@article{pawlak2016spoon,
		title={Spoon: A library for implementing analyses and transformations of java source code},
		author={Pawlak, Renaud and Monperrus, Martin and Petitprez, Nicolas and Noguera, Carlos and Seinturier, Lionel},
		journal={Software: Practice and Experience},
		volume={46},
		number={9},
		pages={1155--1179},
		year={2016},
		publisher={Wiley Online Library}
	}
	
	@article{someoliayi2022sorald,
		title={Sorald: Automatic Patch Suggestions for SonarQube Static Analysis Violations},
		author={Someoliayi, Khashayar Etemadi and Harrand, Nicolas Yves Maurice and Larsen, Simon and Adzemovic, Haris and Phu, Henry Luong and Verma, Ashutosh and Madeiral, Fernanda and Wikstrom, Douglas and Monperrus, Martin},
		journal={IEEE Transactions on Dependable and Secure Computing},
		year={2022},
		publisher={IEEE}
	}
	
	@inproceedings{allamanis2015suggesting,
		title={Suggesting accurate method and class names},
		author={Allamanis, Miltiadis and Barr, Earl T and Bird, Christian and Sutton, Charles},
		booktitle={Proceedings of the 2015 10th joint meeting on foundations of software engineering},
		pages={38--49},
		year={2015}
	}
	
	@article{alon2019code2vec,
		title={code2vec: Learning distributed representations of code},
		author={Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
		journal={Proceedings of the ACM on Programming Languages},
		volume={3},
		number={POPL},
		pages={1--29},
		year={2019},
		publisher={ACM New York, NY, USA}
	}
	
	@article{chicco2020advantages,
		title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
		author={Chicco, Davide and Jurman, Giuseppe},
		journal={BMC genomics},
		volume={21},
		number={1},
		pages={1--13},
		year={2020},
		publisher={BioMed Central}
	}
	@book{thompson2012sampling,
		title={Sampling},
		author={Thompson, Steven K},
		volume={755},
		year={2012},
		publisher={John Wiley \& Sons},
		pages={141--156}
	}
\end{filecontents}
\addbibresource{\jobname.bib}

\usepackage{hyperref}

\author{Lukas Krodinger}
\title{Java Code Readability Study}
\degreeprogramme{M.Sc. Computer Science}
\matrnumber{89801}
\supervisor{Prof. Dr. Gordon Fraser}
%\external{Prof.~John Doe,~PhD}
\advisor{Lisa Griebl}
\department{Faculty of Computer Science and Mathematics}
\institute{Chair of Software Engineering II}
\location{Passau}


%%%%%%%%%%%%%%%%%%%%%%
% Installed Packages %
%%%%%%%%%%%%%%%%%%%%%%
% For java code embeddings
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\usepackage{listings}
\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	showstringspaces=true,
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

% Start counting of chapters with 1
\usepackage{chngcntr}
\counterwithout{chapter}{section}
\counterwithin{chapter}{part}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thechapter}{\arabic{chapter}.0}

% Footnotes
\newcounter{urlfootnote}
\newcommand{\onecurl}[2]{%
	\stepcounter{urlfootnote}%
	\expandafter\def\csname urlfootnote:#1\endcsname{\theurlfootnote}%
	\footnote{\label{url:#1}\url{#1}, accessed: #2}%
}
\usepackage{etoolbox}
\newcommand{\curl}[2]{%
	\ifcsdef{urlfootnote:#1}{%
		\textsuperscript{\ref{url:#1}}%
	}{%
		\onecurl{#1}{#2}%
	}%
}

% Indentation of "Assumption 1" etc. in enumerate
\usepackage{enumitem}

% Nice skips between paragraphs
\usepackage{parskip} 

\begin{document}
	
	\frontmatter
	
	\maketitle
	
	\mainmatter
	
\section{Research Objective}
% Overall goal
The survey aims to evaluate the accuracy of with heuristics predicted readability of Java code snippets. 
% Alternative goal:
% The objective of the survey is Verifying the assumed readability for Java source code snippet.
% For each of the snippets, a certain readability was assumed previously in an automated manner.
% Readability
We define readability as a subjective impression of the difficulty of code while trying to understand it~\cite{posnett2011simpler, buse2009learning}.
% Code snippets
The first part of the code snippets are selected from GitHub repositories with assumed high code quality.
The second part is generated by Readability Decreasing Heuristics. That is, heuristics are used to manipulate the code snippets of the first part to make them less readable.
The survey goal is to verify the following two assumptions:
\begin{enumerate}
	\item \label{well-readable-assumption} \textbf{(well-readable-assumption)} The selected repositories contain only well readable code.
	\item \label{poorly-readable-assumption} \textbf{(poorly-readable-assumption)} After the manipulations, the code is poorly readable.
\end{enumerate}

% \subsection{Organization of those conducting the survey}
The survey is conducted by the Chair of Software Engineering II\footnote{https://www.fim.uni-passau.de/en/chair-for-software-engineering-ii/} of the University of Passau under the supervision of Lukas Krodinger.
%\subsection{Methods for Distribution}
% Prolific 
%Each code snippet is rated by a five point Likert scale~\cite{likert1932technique}. 
The survey will be conducted using Prolific\footnote{https://www.prolific.co/}. This survey summary is created according to the guideline of \citeauthor{linaker2015guidelines}~\cite{linaker2015guidelines}.

\section{Target Population} \label{target-population}
The target population is split into two groups:
\begin{itemize}
	\item \label{students} \textbf{(students)}
	Computer science students with Java experience
	% TODO: If only those, no inference to all programmers, no conclusions to general readability possible!
	\item \label{programmers} \textbf{(programmers)} Java programmers in industry
\end{itemize}

To be more precise, the target population of the survey is as follows:
\begin{center}
	\begin{tabular}{|l|l|}
		\hline
		Target Audience      & Java programmers \\
		Unit of Observation  & Java programmers \\
		Unit of Analysis     & Java programmers \\
		Search Unit %         & Manually selected computer science students /\\
		& Selected by Prolific (Programming Languages: Java)\\
		Source of Sampling   & %University Passau / 
		Prolific \\
		\hline
	\end{tabular}
\end{center}

%Search Unit          & \makecell{Manually selected computer science students /\\ participants selected by Prolific} \\
%Source of Sampling   & University Passau / Prolific \\

\section{Survey}
% The survey consists of three pages. The first page is devoted to Prolific-specific questions and questionnaire attributes, as described in section~\ref{questionnaire-attributes}. The second and third page each contain ten code snippets to be evaluated by the participants.
% In total, we will create 20 unique questionnaires, with each questionnaire being rated by 10 individual participants. Consequently, we will generate a dataset of 400 code snippets. The expected expenditure for this survey, involving 200 participants via the Prolific platform, is estimated to be around ‚Ç¨500.

The first part of the survey is devoted to Prolific-specific questions and questionnaire attributes, as described in section~\ref{questionnaire-attributes}. The second part of the survey consists of 20 code snippets to be evaluated by the participants.
In total, we will create 20 unique questionnaires, with each questionnaire being rated by 10 individual participants. Consequently, we will generate a dataset of 400 code snippets. The expected expenditure for this survey, involving 200 participants via the Prolific platform, is estimated to be around ‚Ç¨500.

\subsection{Sample Size}
To get accurate study results, multiple participants have to rate the same code snippets. We specify a size of ten participants per code snippet, similar to \citeauthor{scalabrino2016improving}~\cite{scalabrino2016improving}. According to an online calculator\footnote{https://www.calculator.net/sample-size-calculator.html} we end up with a margin of error of 33.99\% at a confidence of 95\%. This means that we are 95\% confident that the actual readability of a code snippet is within 33.99\% of the survey result score. With a probability of 5\%, the readability of a code snippet differs by more than 33.99\% from the score of the survey result. On a Likert scale ranging from 1 to 5, the mean of the survey results is between 1.0 and 5.0. In this context, an offset of 33.99\% is an offset of more than one rating point~\cite{likert1932technique}. We need to consider this when evaluating our the gathered data.

% In order to calculate the required sample size for a single code snippet, we need to specify a confidence level and a margin of error.
% We have relatively small requirements to both, and therewith set the confidence level to 95\% and the margin of error to 25\%. Using an online calculator\footnote{https://www.calculator.net/sample-size-calculator.html} we find out, that we need 16 participants per code snippet in order to achieve this.

% In order to ensure reliable results, we calculate the number of participants needed per code snippet as follows:

% \begin{equation}
	%     n = \frac{Z^2*P*(1 - P)}{E^2}
	% \end{equation}
% \begin{itemize}
	%     \item n = Required sample size
	%     \item Z = Z-score for the desired confidence level
	%     \item P = Estimated proportion of respondents choosing any specific option
	%     \item E = Margin of error
	% \end{itemize}

% For choosing Z, P and E we consider that we have 5 options per question (variability of options). A confidence level of 95\% is sufficient. 
% Given this, we define the required values as follows:
% \begin{itemize}
	%     \item Margin of Error: E = 5\%
	%     \item Z-Score: Z = 1.96
	%     \item P = 50\%
	% \end{itemize}

\subsection{Time Requirement}
The first part (Prolific-specific questions and questionnaire attributes) will take the participant about one minute. The average time for rating a snippet is estimated with 30 seconds. Thus, rating 20 snippets will take about 10 minutes. The total time for completing the survey is estimated with 11 minutes.

\subsection{Sampling Design}
% Manually selected computer science students from the University of Passau will participate in the survey.
The survey participants will be selected using Prolific. The only restriction for the participants will be that they must be familiar with Java.

% TODO: Quelle
% Paper on sampling: https://www.researchgate.net/profile/Anita-Acharya-2/publication/256446902_Sampling_Why_and_How_of_it_Anita_S_Acharya_Anupam_Prakash_Pikee_Saxena_Aruna_Nigam/links/0c960527c82d449788000000/Sampling-Why-and-How-of-it-Anita-S-Acharya-Anupam-Prakash-Pikee-Saxena-Aruna-Nigam.pdf
A big part of our code snippets might be getters and setters. However, we do not want to mostly evaluate getters and setters in our study. Therefore, we will use stratified sampling~\cite{thompson2012sampling}. Thus, code snippets are split into groups with high similarity, so-called strata. When we then randomly sample within the strata, we make sure to not only sample getters and setters.

To create the required stratas we need to measure the similarity of code snippets. \citeauthor{scalabrino2016improving} developed a tool to generate various metrics for Java source code~\cite{scalabrino2016improving}. We will use this tool on each code snippet and create a code vector for each snippet. The Euclidean distance between these vectors provides an estimation of source code similarity, which will be used to create the strata.

\subsection{Internal Questions} % Research Questions
The internal research questions are as follows:
\begin{itemize}
	\item Does the well-readable-assumption (\ref{well-readable-assumption}) hold?
	\item Does the poorly-readable-assumption (\ref{poorly-readable-assumption}) hold?
\end{itemize}
The results of the questions are equally important, and thus none of them is prioritized over the other.

\subsection{Questionnaire Attributes} \label{questionnaire-attributes}
The survey neither contains demographic questions nor filter questions. Besides the readability questions, each user is asked the following dependent question: "How would you describe your familiarity with Java?". The user can answer within a five point Likert scale: expert (5), advanced (4), intermediate (3), beginner (2), novice (1).

%     % Alternativ:
% D = Dependent. I = Independent.
% \begin{itemize}
	
	%     % \item (I) Participant Name - Open Ended
	
	%     % \item (D) What is your profession? - TODO: Answer options?
	%     \item (D) How familiar are you with Java? - Answer (4 point Likert scale): very familiar, familiar, not very familiar, not familiar at all
	%     % Alternativ:
	%     % \item (D) How would you rate your own expertise in Java?
	
	%     % More background related questions?
	%     % \item (D) Do you have an academical degree in computer science or similar? - No, Yes - B.Sc., Yes - M.Sc, Yes - Dr., Yes Prof.
	%     % \item (D) How long (if any) did you computer science? - Interval response
	%     % \item (D) How long (if any) did you work at a computer science related job with java?
	%     % \item (D) If you did work, how many hours per week?
	
	%     % (I) Gender, (I) age? 
	
	% \end{itemize}

% Last question?
% Face Validity: Ask questioners about validity

\subsection{Survey Questions} % Questionnaire- Opinion
For each code snippet, the following closed-ended question is given: "How do you rate the readability of this code?". The answers follow the Likert Scale: very high (5), high (4), medium (3), low (2), very low (1).

\subsection{Survey Result Evaluation}
Once a survey is completed by a participant, the survey result is evaluated to make sure the participants did not choose answers at random. The code snippet rating answers are checked for plausibility. For example, if a code snippet with expected rating 5 (by the heuristics and/or by other participants) is rated with 2 or less, this is an indication that the participant did not fill out the survey carefully. Several such indications lead to the exclusion of the participant and his answers.

\section{Software}
We will use Prolific to pay participants for completing our survey. Our participants will fill out the survey using TODO: ADD TOOL.

\section{Survey evaluation groups} \label{sec:survey-evaluation}
We will evaluate the survey construction by presenting it to the following groups:
\begin{itemize}
	% \item Survey and questionnaire design experts
	\item Subject-matter experts
	\item Focus group (discussion with about 7 people)
	\item Pilot survey group
\end{itemize}

\section{Threads to Validity and Reliability} % Quality assurance
% Possible threads to the survey might be: 
% \begin{itemize}
	%     \item Ill-defined target population
	%     \item Sampling method (stratified sampling)
	%     \item Too few responses for conclusions
	% \end{itemize}
% To avoid those threads, we come up with the following countermeasures:
% To overcome ill-defined target population we define our target population clearly in this document, and we evaluate the document (see section~\ref{sec:survey-evaluation}) before conducting the survey. This will also ensure content and construct validity. 
% Our sampling method is well-defined and proven in practice.
% We scale our survey to an appropriate size to have enough responses. Therewith, we avoid taking conclusions from too little responses.
This section addresses potential threats to the survey's validity and reliability. These threats include:
\begin{enumerate}
	\item \textbf{Ill-defined Target Population:} Ensuring a well-defined target population is critical to the survey's quality. To mitigate this threat, we clearly define our target population within this document (see section~\ref{target-population}). Additionally, we conduct a pre-survey evaluation (see section~\ref{sec:survey-evaluation}) to ensure the adequacy of our target population definition. Thereby, we enhance content and construct validity.
	
	\item \textbf{Sampling Method (Stratified Sampling):} Our chosen sampling method is well-defined and proven in practice. This approach ensures that our sample represents all parts of the population under investigation. This is improving the survey's external validity.
	
	\item \textbf{Insufficient Responses for Drawing Conclusions:} To prevent drawing conclusions from an insufficient number of responses, we scale our survey to an appropriate size. This guarantees that we collect a substantial volume of responses, allowing for robust statistical analysis.
\end{enumerate}

% \subsection{Threads to Validity}
% % Face validity: See Questions
% To ensure content and construct validity, the survey will be reviewed by a focus group and presented to a pilot group before being distributed to the target group. We will measure the differences of the responses of the two different target population groups: students and programmers. Thereby, we check criterion validity. 

% \subsection{Threads to Reliability}
% % Reliability/External Validity/Generalizability
% A small sample of the target group will be asked to conduct the survey again after a few months. In the second survey, we will ask the same questions in a different order. We will then measure the correlation between the results of the two surveys. The goal is to have a correlation greater than $0.7$. If achieved, test-reset reliability is given.
% By conducting the survey fully online, we can be sure that observers do not interfere with the participants. Therewith, inter-observer reliability is given.

\section{Schedule}
% The survey concept proposal is finished latest till the November 2023. The survey is then presented to the survey evaluation groups and revised till January 2024. The survey will be conducted in February 2024 and the results are evaluated in March 2024.
You can find the survey schedule in table~\ref{tab:survey-timeline}.

\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Task} & \textbf{Timeline (finished latest)} \\
		\hline
		Survey concept proposal & November 2023 \\
		\hline
		Presentation to survey evaluation groups & December 2024 \\
		\hline
		Survey conduction & January 2024 \\
		\hline
		Results evaluation & February 2024 \\
		\hline
	\end{tabular}
	\caption{Survey Timeline}
	\label{tab:survey-timeline}
\end{table}

% TODO: Move? Maybe to first section?
\section{Expected Results}
We expect that the averaged ratings of the selected code snippets matches the predicted code readability to a certain extent. We do not expect an exact match.
We measure this extent by applying the mean as a measure of central tendency and standard deviation as a measure of variability, as proposed by \citeauthor{linaker2015guidelines}~\cite{linaker2015guidelines}. We make those measurements with respect to stratified sampling.
Our null hypothesis is, that both assumptions (assumption~\ref{well-readable-assumption}~and~\ref{poorly-readable-assumption}) hold, i.e., that the expected readability does not deviate too much from the mean of the survey participants.
We will visualize our results by generating a bar chart for each evaluated snippet. We will also provide a visualization that summarizes the mean deviations of participants' ratings from expected readability for all snippets.
	
	\backmatter
	
	\printbibliography
	
\end{document}
