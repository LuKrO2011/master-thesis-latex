%%%%%%%%%%%%%%%%%
% From Template %
%%%%%%%%%%%%%%%%%

\documentclass[%
class=scrreprt,
chapterprefix=false,%
open=right,%
twoside=false,%
paper=a4,%
logofile={Logo\_zentral\_farbig\_EN.png},%
thesistype=masterproposal,%
UKenglish,%
]{se2thesis}
\listfiles
\usepackage[ngerman,main=UKenglish]{babel}
\usepackage{blindtext}
\usepackage[%
csquotes=true,%
booktabs=true,%
siunitx=true,%
minted=true,%
selnolig=true,%
widowcontrol=false,%
microtype=true,%
% biblatex=true,%
cleveref=true,%
]{se2packages}

% Own change (from Lisa)
% Changing citeauthor to use et el.
\usepackage[maxcitenames=2,backend=biber]{biblatex}
% End own change

\begin{filecontents}{\jobname.bib}
	@article{linaker2015guidelines,
		author = {Lin√•ker, Johan and Sulaman, Sardar and Host, Martin and de Mello, Rafael},
		year = {2015},
		month = {05},
		pages = {},
		title = {Guidelines for Conducting Surveys in Software Engineering}
	}
	
	@article{boehm2001defect,
		title={Defect reduction top 10 list},
		author={Boehm, Barry and Basili, Victor R},
		journal={Computer},
		volume={34},
		number={1},
		pages={135--137},
		year={2001}
	}
	
	@article{buse2009learning,
		title={Learning a metric for code readability},
		author={Buse, Raymond PL and Weimer, Westley R},
		journal={IEEE Transactions on software engineering},
		volume={36},
		number={4},
		pages={546--558},
		year={2009},
		publisher={IEEE}
	}
	
	@inproceedings{aggarwal2002integrated,
		title={An integrated measure of software maintainability},
		author={Aggarwal, Krishan K and Singh, Yogesh and Chhabra, Jitender Kumar},
		booktitle={Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No. 02CH37318)},
		pages={235--241},
		year={2002},
		organization={IEEE}
	}
	
	@inproceedings{fakhoury2019improving,
		title={Improving source code readability: Theory and practice},
		author={Fakhoury, Sarah and Roy, Devjeet and Hassan, Adnan and Arnaoudova, Vernera},
		booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
		pages={2--12},
		year={2019},
		organization={IEEE}
	}
	
	@article{scalabrino2018comprehensive,
		title={A comprehensive model for code readability},
		author={Scalabrino, Simone and Linares-V{\'a}squez, Mario and Oliveto, Rocco and Poshyvanyk, Denys},
		journal={Journal of Software: Evolution and Process},
		volume={30},
		number={6},
		pages={e1958},
		year={2018},
		publisher={Wiley Online Library}
	}
	
	@inproceedings{posnett2011simpler,
		title={A simpler model of software readability},
		author={Posnett, Daryl and Hindle, Abram and Devanbu, Premkumar},
		booktitle={Proceedings of the 8th working conference on mining software repositories},
		pages={73--82},
		year={2011}
	}
	
	@inproceedings{dorn2012general,
		title={A General Software Readability Model},
		author={Jonathan Dorn},
		year={2012},
		url={https://api.semanticscholar.org/CorpusID:14098740}
	}
	
	@inproceedings{scalabrino2017automatically,
		title={Automatically assessing code understandability: How far are we?},
		author={Scalabrino, Simone and Bavota, Gabriele and Vendome, Christopher and Linares-V{\'a}squez, Mario and Poshyvanyk, Denys and Oliveto, Rocco},
		booktitle={2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
		pages={417--427},
		year={2017},
		organization={IEEE}
	}
	
	@inproceedings{daka2015modeling,
		title={Modeling readability to improve unit tests},
		author={Daka, Ermira and Campos, Jos{\'e} and Fraser, Gordon and Dorn, Jonathan and Weimer, Westley},
		booktitle={Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
		pages={107--118},
		year={2015}
	}
	
	@article{mi2022towards,
		title={Towards using visual, semantic and structural features to improve code readability classification},
		author={Mi, Qing and Hao, Yiqun and Ou, Liwei and Ma, Wei},
		journal={Journal of Systems and Software},
		volume={193},
		pages={111454},
		year={2022},
		publisher={Elsevier}
	}
	
	@inproceedings{mi2018gamification,
		title={A gamification technique for motivating students to learn code readability in software engineering},
		author={Mi, Qing and Keung, Jacky and Mei, Xiupei and Xiao, Yan and Chan, WK},
		booktitle={2018 International Symposium on Educational Technology (ISET)},
		pages={250--254},
		year={2018},
		organization={IEEE}
	}
	
	@article{mi2018improving,
		title={Improving code readability classification using convolutional neural networks},
		author={Mi, Qing and Keung, Jacky and Xiao, Yan and Mensah, Solomon and Gao, Yujin},
		journal={Information and Software Technology},
		volume={104},
		pages={60--71},
		year={2018},
		publisher={Elsevier}
	}
	
	@book{brooks1987no,
		title={No silver bullet},
		author={Brooks, Frederick and Kugler, H},
		year={1987},
		publisher={April}
	}
	
	@article{loriot2022styler,
		title={Styler: learning formatting conventions to repair Checkstyle violations},
		author={Loriot, Benjamin and Madeiral, Fernanda and Monperrus, Martin},
		journal={Empirical Software Engineering},
		volume={27},
		number={6},
		pages={149},
		year={2022},
		publisher={Springer}
	}
	
	@inproceedings{yasunaga2020graph,
		title={Graph-based, self-supervised program repair from diagnostic feedback},
		author={Yasunaga, Michihiro and Liang, Percy},
		booktitle={International Conference on Machine Learning},
		pages={10799--10808},
		year={2020},
		organization={PMLR}
	}
	
	@inproceedings{xu2019method,
		title={Method name suggestion with hierarchical attention networks},
		author={Xu, Sihan and Zhang, Sen and Wang, Weijing and Cao, Xinya and Guo, Chenkai and Xu, Jing},
		booktitle={Proceedings of the 2019 ACM SIGPLAN workshop on partial evaluation and program manipulation},
		pages={10--21},
		year={2019}
	}
	
	@inproceedings{allamanis2016convolutional,
		title={A convolutional attention network for extreme summarization of source code},
		author={Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
		booktitle={International conference on machine learning},
		pages={2091--2100},
		year={2016},
		organization={PMLR}
	}
	
	@article{hestness2017deep,
		title={Deep learning scaling is predictable, empirically},
		author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
		journal={arXiv preprint arXiv:1712.00409},
		year={2017}
	}
	
	@inproceedings{liu2019learning,
		title={Learning to spot and refactor inconsistent method names},
		author={Liu, Kui and Kim, Dongsun and Bissyand{\'e}, Tegawend{\'e} F and Kim, Taeyoung and Kim, Kisub and Koyuncu, Anil and Kim, Suntae and Le Traon, Yves},
		booktitle={2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
		pages={1--12},
		year={2019},
		organization={IEEE}
	}
	
	@article{deimel1985uses,
		title={The uses of program reading},
		author={Deimel Jr, Lionel E},
		journal={ACM SIGCSE Bulletin},
		volume={17},
		number={2},
		pages={5--14},
		year={1985},
		publisher={ACM New York, NY, USA}
	}
	
	@inproceedings{raymond1991reading,
		title={Reading source code.},
		author={Raymond, Darrell R},
		booktitle={CASCON},
		volume={91},
		pages={3--16},
		year={1991}
	}
	
	@article{rugaber2000use,
		title={The use of domain knowledge in program understanding},
		author={Rugaber, Spencer},
		journal={Annals of Software Engineering},
		volume={9},
		number={1-4},
		pages={143--192},
		year={2000},
		publisher={Springer}
	}
	
	@article{likert1932technique,
		title={A technique for the measurement of attitudes.},
		author={Likert, Rensis},
		journal={Archives of psychology},
		year={1932}
	}
	
	@inproceedings{wyrich2019towards,
		title={Towards an autonomous bot for automatic source code refactoring},
		author={Wyrich, Marvin and Bogner, Justus},
		booktitle={2019 IEEE/ACM 1st international workshop on bots in software engineering (BotSE)},
		pages={24--28},
		year={2019},
		organization={IEEE}
	}
	
	@article{pawlak2016spoon,
		title={Spoon: A library for implementing analyses and transformations of java source code},
		author={Pawlak, Renaud and Monperrus, Martin and Petitprez, Nicolas and Noguera, Carlos and Seinturier, Lionel},
		journal={Software: Practice and Experience},
		volume={46},
		number={9},
		pages={1155--1179},
		year={2016},
		publisher={Wiley Online Library}
	}
	
	@article{someoliayi2022sorald,
		title={Sorald: Automatic Patch Suggestions for SonarQube Static Analysis Violations},
		author={Someoliayi, Khashayar Etemadi and Harrand, Nicolas Yves Maurice and Larsen, Simon and Adzemovic, Haris and Phu, Henry Luong and Verma, Ashutosh and Madeiral, Fernanda and Wikstrom, Douglas and Monperrus, Martin},
		journal={IEEE Transactions on Dependable and Secure Computing},
		year={2022},
		publisher={IEEE}
	}
	
	@inproceedings{allamanis2015suggesting,
		title={Suggesting accurate method and class names},
		author={Allamanis, Miltiadis and Barr, Earl T and Bird, Christian and Sutton, Charles},
		booktitle={Proceedings of the 2015 10th joint meeting on foundations of software engineering},
		pages={38--49},
		year={2015}
	}
	
	@article{alon2019code2vec,
		title={code2vec: Learning distributed representations of code},
		author={Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
		journal={Proceedings of the ACM on Programming Languages},
		volume={3},
		number={POPL},
		pages={1--29},
		year={2019},
		publisher={ACM New York, NY, USA}
	}
	
	@article{chicco2020advantages,
		title={The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation},
		author={Chicco, Davide and Jurman, Giuseppe},
		journal={BMC genomics},
		volume={21},
		number={1},
		pages={1--13},
		year={2020},
		publisher={BioMed Central}
	}
	@book{thompson2012sampling,
		title={Sampling},
		author={Thompson, Steven K},
		volume={755},
		year={2012},
		publisher={John Wiley \& Sons},
		pages={141--156}
	}
\end{filecontents}
\addbibresource{\jobname.bib}

\usepackage{hyperref}

\author{Lukas Krodinger}
\title{Java Code Readability Study}
\degreeprogramme{M.Sc. Computer Science}
\matrnumber{89801}
\supervisor{Prof. Dr. Gordon Fraser}
%\external{Prof.~John Doe,~PhD}
\advisor{Lisa Griebl}
\department{Faculty of Computer Science and Mathematics}
\institute{Chair of Software Engineering II}
\location{Passau}


%%%%%%%%%%%%%%%%%%%%%%
% Installed Packages %
%%%%%%%%%%%%%%%%%%%%%%
% For java code embeddings
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\usepackage{listings}
\lstset{frame=tb,
	language=Java,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	showstringspaces=true,
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}

% Start counting of chapters with 1
\usepackage{chngcntr}
\counterwithout{chapter}{section}
\counterwithin{chapter}{part}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thechapter}{\arabic{chapter}.0}

% Footnotes
\newcounter{urlfootnote}
\newcommand{\onecurl}[2]{%
	\stepcounter{urlfootnote}%
	\expandafter\def\csname urlfootnote:#1\endcsname{\theurlfootnote}%
	\footnote{\label{url:#1}\url{#1}, accessed: #2}%
}
\usepackage{etoolbox}
\newcommand{\curl}[2]{%
	\ifcsdef{urlfootnote:#1}{%
		\textsuperscript{\ref{url:#1}}%
	}{%
		\onecurl{#1}{#2}%
	}%
}

% Indentation of "Assumption 1" etc. in enumerate
\usepackage{enumitem}

% Nice skips between paragraphs
\usepackage{parskip} 

\begin{document}
	
	\frontmatter
	
	\maketitle
	
	\mainmatter
	
\section{Research Objective}
% Overall goal
The survey aims to evaluate the accuracy of Java code snippet readability predicted with heuristics.
% Alternative goal:
% The objective of the survey is Verifying the assumed readability for Java source code snippet.
% For each of the snippets, a certain readability was assumed previously in an automated manner.
% Readability
% We define readability as a subjective impression of the difficulty of code while trying to understand it~\cite{posnett2011simpler, buse2009learning}.
% Code snippets
The first part of the code snippets are selected from GitHub repositories with assumed high code quality.
The second part is generated by Readability Decreasing Heuristics. That is, heuristics are used to manipulate the code snippets of the first part to make them less readable.
The main objective of the survey is to validate the following two assumptions:
\begin{enumerate}
	\item \label{well-readable-assumption} \textbf{Assumption 1 (well-readable-assumption)} The selected repositories contain only well readable code.
	
	\item \label{poorly-readable-assumption} \textbf{Assumption 2 (poorly-readable-assumption)} After the manipulations, the code is poorly readable.
\end{enumerate}

% \subsection{Organization of those conducting the survey}
The survey is conducted by the Chair of Software Engineering II\footnote{https://www.fim.uni-passau.de/en/chair-for-software-engineering-ii/} of the University of Passau under the supervision of Lukas Krodinger.
%\subsection{Methods for Distribution}
% Prolific 
%Each code snippet is rated by a five point Likert scale~\cite{likert1932technique}. 
Survey participants are selected using Prolific\footnote{https://www.prolific.co/}, and conducted online using Tien Duc Nguyen's Code Annotation Tool. This survey summary was prepared following the guidelines of \citeauthor{linaker2015guidelines}~\cite{linaker2015guidelines}.

\section{Target Population} \label{target-population}
The target population is split into two groups:
\begin{itemize}
	\item \label{students} \textbf{Students}:
	Computer science students with Java experience
	% TODO: If only those, no inference to all programmers, no conclusions to general readability possible!
	\item \label{programmers} \textbf{Programmers}: Java programmers in industry
\end{itemize}

To be more precise, the target population of the survey is as follows:
\begin{center}
	\begin{tabular}{|l|l|}
		\hline
		Target Audience      & Java programmers \\
		Unit of Observation  & Java programmers \\
		Unit of Analysis     & Java programmers \\
		Search Unit %         & Manually selected computer science students /\\
		& Selected by Prolific (Programming Languages: Java)\\
		Source of Sampling   & %University Passau / 
		Prolific \\
		\hline
	\end{tabular}
\end{center}

%Search Unit          & \makecell{Manually selected computer science students /\\ participants selected by Prolific} \\
%Source of Sampling   & University Passau / Prolific \\

\section{Survey}
% The survey consists of three pages. The first page is devoted to Prolific-specific questions and questionnaire attributes, as described in section~\ref{questionnaire-attributes}. The second and third page each contain ten code snippets to be evaluated by the participants.
% In total, we will create 20 unique questionnaires, with each questionnaire being rated by 10 individual participants. Consequently, we will generate a dataset of 400 code snippets. The expected expenditure for this survey, involving 200 participants via the Prolific platform, is estimated to be around ‚Ç¨500.

The first part of the survey is devoted to Prolific-specific questions and questionnaire attributes, as described in Section~\ref{questionnaire-attributes}. The second part of the survey consists of 20 code snippets to be evaluated by the participants.
In total, we create 20 unique questionnaires, with each questionnaire being rated by 10 individual participants. Consequently, we generate a dataset of 400 code snippets. The expected expenditure for this survey, involving 200 participants via the Prolific platform, is estimated to be around ‚Ç¨500.

\subsection{Sample Size}
To get accurate study results, multiple participants have to rate the same code snippets. We specify a size of ten participants per code snippet, similar to \citeauthor{scalabrino2018comprehensive}~\cite{scalabrino2018comprehensive}. According to an online calculator\footnote{https://www.calculator.net/sample-size-calculator.html} we end up with a margin of error of 33.99\% at a confidence of 95\%. This means that we are 95\% confident that the actual readability of a code snippet is within 33.99\% of the survey result score. With a probability of 5\%, the readability of a code snippet differs by more than 33.99\% from the score of the survey result. On a Likert scale ranging from 1 to 5, the mean of the survey results is between 1.0 and 5.0. In this context, an offset of 33.99\% is an offset of more than one rating point~\cite{likert1932technique}. We need to consider this when evaluating our the gathered data.

% In order to calculate the required sample size for a single code snippet, we need to specify a confidence level and a margin of error.
% We have relatively small requirements to both, and therewith set the confidence level to 95\% and the margin of error to 25\%. Using an online calculator\footnote{https://www.calculator.net/sample-size-calculator.html} we find out, that we need 16 participants per code snippet in order to achieve this.

% In order to ensure reliable results, we calculate the number of participants needed per code snippet as follows:

% \begin{equation}
	%     n = \frac{Z^2*P*(1 - P)}{E^2}
	% \end{equation}
% \begin{itemize}
	%     \item n = Required sample size
	%     \item Z = Z-score for the desired confidence level
	%     \item P = Estimated proportion of respondents choosing any specific option
	%     \item E = Margin of error
	% \end{itemize}

% For choosing Z, P and E we consider that we have 5 options per question (variability of options). A confidence level of 95\% is sufficient. 
% Given this, we define the required values as follows:
% \begin{itemize}
	%     \item Margin of Error: E = 5\%
	%     \item Z-Score: Z = 1.96
	%     \item P = 50\%
	% \end{itemize}

\subsection{Time Requirement}
The first part (Prolific-specific questions and questionnaire attributes) takes the participant about one minute. The average time for rating a snippet is estimated with 30 seconds. Thus, rating 20 snippets takes about 10 minutes. The total time for completing one questionnaire is estimated with 11 minutes.

\subsection{Sampling Design} \label{sampling-design}
% Manually selected computer science students from the University of Passau will participate in the survey.
The survey participants are selected using Prolific. The only restriction for the participants is that they must be familiar with Java.

% TODO: Quelle
% Paper on sampling: https://www.researchgate.net/profile/Anita-Acharya-2/publication/256446902_Sampling_Why_and_How_of_it_Anita_S_Acharya_Anupam_Prakash_Pikee_Saxena_Aruna_Nigam/links/0c960527c82d449788000000/Sampling-Why-and-How-of-it-Anita-S-Acharya-Anupam-Prakash-Pikee-Saxena-Aruna-Nigam.pdf
A big part of our code snippets might be getters and setters. However, we do not want to mostly evaluate getters and setters in our study. Therefore, we use stratified sampling~\cite{thompson2012sampling}. Thus, code snippets are split into groups with high similarity, so-called strata. When we then randomly sample within the strata, we make sure to not only sample getters and setters.

To create the required stratas we need to measure the similarity of code snippets. \citeauthor{scalabrino2018comprehensive} developed a tool to generate various metrics for Java source code~\cite{scalabrino2018comprehensive}. We use this tool on each code snippet and create a code vector for each snippet. The Euclidean distance between these vectors provides an estimation of source code similarity, which is used to create the strata.

After sampling the downloaded code snippets in this way, we want to include the same snippets after performing manipulations. There are several variants with differences in type and frequency for each downloaded snippet. Multiple snippets from the same source snippet (including the source snippet itself) are not in the same questionnaire. Thus, a user cannot compare the snippets with each other. This does not diminish the inferences that can be made about all the data based on stratified sampling. However, conclusions can be drawn about the effectiveness of different heuristics.

\subsection{Internal Questions} % Research Questions
The internal research questions are as follows:
\begin{itemize}
	\item Does the well-readable-assumption (Assumption \ref{well-readable-assumption}) hold?
	\item Does the poorly-readable-assumption (Assumption \ref{poorly-readable-assumption}) hold?
\end{itemize}

The results for these questions are equally important, and thus none of them is prioritized over the other.
To answer them, the assumptions %(Assumption~\ref{well-readable-assumption}~and~\ref{poorly-readable-assumption})
are considered as hypotheses along with the following associated null hypotheses:
\begin{itemize}
	\item For Assumption~\ref{well-readable-assumption}: The mined code exhibits a normal distribution of readability scores.
	\item For Assumption~\ref{poorly-readable-assumption}: The readability of code does not significantly deteriorate compared to the original code snippet.
\end{itemize}



Another goal of the study is to find estimates of which heuristics need to be applied how often to make the code less readable to some degree. This helps us to further adjust the settings of our generation approach for poorly readable code.

The goal of our study is \textbf{not} to label code snippets by readability. If our assumptions  (Assumption~\ref{well-readable-assumption}~and~\ref{poorly-readable-assumption})
are confirmed, we can use them to make inferences about the readability of all code snippets generated with our approach. This is a crucial difference from comparable other studies~\cite{buse2009learning, dorn2012general, scalabrino2018comprehensive}.

\subsection{Questionnaire Attributes} \label{questionnaire-attributes}
The survey neither contains demographic questions nor filter questions. Besides the readability questions, each user is asked the following dependent question: "How would you describe your familiarity with Java?". The user can answer within a five point Likert scale: expert (5), advanced (4), intermediate (3), beginner (2), novice (1).

%     % Alternativ:
% D = Dependent. I = Independent.
% \begin{itemize}
	
	%     % \item (I) Participant Name - Open Ended
	
	%     % \item (D) What is your profession? - TODO: Answer options?
	%     \item (D) How familiar are you with Java? - Answer (4 point Likert scale): very familiar, familiar, not very familiar, not familiar at all
	%     % Alternativ:
	%     % \item (D) How would you rate your own expertise in Java?
	
	%     % More background related questions?
	%     % \item (D) Do you have an academical degree in computer science or similar? - No, Yes - B.Sc., Yes - M.Sc, Yes - Dr., Yes Prof.
	%     % \item (D) How long (if any) did you computer science? - Interval response
	%     % \item (D) How long (if any) did you work at a computer science related job with java?
	%     % \item (D) If you did work, how many hours per week?
	
	%     % (I) Gender, (I) age? 
	
	% \end{itemize}

% Last question?
% Face Validity: Ask questioners about validity

\subsection{Survey Questions} % Questionnaire- Opinion
For each code snippet, the following closed-ended question is given: "How do you rate the readability of this code?". The answers follow the Likert Scale: very high (5), high (4), medium (3), low (2), very low (1).

\subsection{Survey Result Evaluation}
Once a survey is completed by a participant, the survey result is evaluated to make sure the participants did not choose answers at random. The code snippet rating answers are checked for plausibility. For example, if a code snippet with expected rating 5 (by the heuristics and/or by other participants) is rated with 2 or less, this is an indication that the participant did not fill out the survey carefully. Several such indications lead to the exclusion of the participants and their answers.

\section{Software}
We use Prolific to pay participants for completing our survey. Our participants fill out the survey using Tien Duc Nguyen's Code Annotation Tool.

\section{Survey evaluation} \label{sec:survey-evaluation}
We evaluate the survey construction by presenting it to the following groups:
\begin{itemize}
	% \item Survey and questionnaire design experts
	\item Subject-matter experts
	\item Focus group (discussion with about 7 people)
	\item Pilot survey group
\end{itemize}

Another way to evaluate our survey on an ongoing basis is to conduct it iteratively. We start with three questionnaires and expand the number based on our conclusions from the first ones. This offers several advantages. Firstly, we can still adjust the survey if problems arise. Secondly, we can start building our pipeline for evaluation earlier. Finally, after each iteration, we can decide whether to include more variants of the same source code snippet or to increase the number of source code snippets instead (see Section \ref{sampling-design}). The first option could give us more insights on how to adjust the settings for our heuristics. The second option could increase the number of stratas involved or allow us to increase the evidence for a particular stratum by including more samples of the respective one.

\section{Threads} % Quality assurance
% Possible threads to the survey might be: 
% \begin{itemize}
	%     \item Ill-defined target population
	%     \item Sampling method (stratified sampling)
	%     \item Too few responses for conclusions
	% \end{itemize}
% To avoid those threads, we come up with the following countermeasures:
% To overcome ill-defined target population we define our target population clearly in this document, and we evaluate the document (see section~\ref{sec:survey-evaluation}) before conducting the survey. This will also ensure content and construct validity. 
% Our sampling method is well-defined and proven in practice.
% We scale our survey to an appropriate size to have enough responses. Therewith, we avoid taking conclusions from too little responses.
This section addresses potential threats to the survey's validity and reliability. These threats include:
\begin{enumerate}
	\item \textbf{Ill-defined Target Population:} Ensuring a well-defined target population is critical to the survey's quality. To mitigate this threat, we clearly define our target population within this document (see Section~\ref{target-population}). Additionally, we conduct a pre-survey evaluation (see Section~\ref{sec:survey-evaluation}) to ensure the adequacy of our target population definition. Thereby, we enhance content and construct validity.
	
	\item \textbf{Sampling Method (Stratified Sampling):} Our chosen sampling method is well-defined and proven in practice. This approach ensures that our sample represents all parts of the population under investigation. This is improving the survey's external validity.
	
	\item \textbf{Insufficient Responses for Drawing Conclusions:} To prevent drawing conclusions from an insufficient number of responses, we scale our survey to an appropriate size. This guarantees that we collect a substantial volume of responses, allowing for robust statistical analysis.
\end{enumerate}

% \subsection{Threads to Validity}
% % Face validity: See Questions
% To ensure content and construct validity, the survey will be reviewed by a focus group and presented to a pilot group before being distributed to the target group. We will measure the differences of the responses of the two different target population groups: students and programmers. Thereby, we check criterion validity. 

% \subsection{Threads to Reliability}
% % Reliability/External Validity/Generalizability
% A small sample of the target group will be asked to conduct the survey again after a few months. In the second survey, we will ask the same questions in a different order. We will then measure the correlation between the results of the two surveys. The goal is to have a correlation greater than $0.7$. If achieved, test-reset reliability is given.
% By conducting the survey fully online, we can be sure that observers do not interfere with the participants. Therewith, inter-observer reliability is given.

\section{Schedule}
% The survey concept proposal is finished latest till the November 2023. The survey is then presented to the survey evaluation groups and revised till January 2024. The survey will be conducted in February 2024 and the results are evaluated in March 2024.
You can find the survey schedule in Table~\ref{tab:survey-timeline}.

\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\textbf{Task} & \textbf{Timeline (finished latest)} \\
		\hline
		Survey concept proposal & October 2023 \\
		\hline
		Presentation to survey evaluation groups & November 2024 \\
		\hline
		Survey conduction & December 2023 \\
		\hline
		Evaluation of results & January 2024 \\
		\hline
	\end{tabular}
	\caption{Survey Timeline}
	\label{tab:survey-timeline}
\end{table}

% TODO: Move? Maybe to first section?
\section{Expected Results}
We expect that the averaged ratings of the selected code snippets matches the predicted code readability to a certain extent. We do not expect an exact match.
We measure this extent by applying the mean as a measure of central tendency and standard deviation as a measure of variability, as proposed by \citeauthor{linaker2015guidelines}~\cite{linaker2015guidelines}. We make those measurements with respect to stratified sampling.\\

The primary objective is to validate our assumptions (Assumption~\ref{well-readable-assumption}~and~\ref{poorly-readable-assumption}). 
The primary objective of this study is to validate a novel dataset. It aims to demonstrate its suitability for training  readability classifiers, despite its heuristic construction. Thus, the study seeks to validate our assumptions (Assumption~\ref{well-readable-assumption}~and~\ref{poorly-readable-assumption}).

After conduction, we visualize our results by generating car charts. We also provide a visualization that summarizes the mean deviations of participants' ratings from expected readability for all snippets.


%\section{Survey Introduction}
%In the following survey we will ask you to rate Java code snippets by their readability. Code readability is the impression of difficulty of code while trying to understand it. Please note, that readability is not the same as understandability or complexity. Consider this example:
%
%\begin{listing}[!ht]
%	\begin{minted}[linenos, frame=lines, framesep=2mm]{java}
%		public static int[] generateFibonacci(int n) {
%			int[] fibonacci = new int[n];
%			fibonacci[0] = 0;
%			fibonacci[1] = 1;
%			
%			for (int i = 2; i < n; i++) {
%				fibonacci[i] = fibonacci[i - 1] + fibonacci[i - 2];
%			}
%			
%			return fibonacci;
%		}
%	\end{minted}
%	\caption[Fibonacci program with high readability and understandability]{Fibonacci program with high readability and understandability}
%	\label{lst:fibonacci-default}
%\end{listing}
%
%\begin{listing}[!ht]
%	\begin{minted}[linenos, frame=lines, framesep=2mm]{java}
%		public static int[] foo(int x) {
%			int[] bar = new int[x];
%			bar[0] = 0;
%			bar[1] = 1;
%			
%			for (int i = 2; i < x; i++) {
%				bar[i] = bar[i - 1] + bar[i - 2];
%			}
%			
%			return bar;
%		}
%	\end{minted}
%	\caption[Fibonacci program with with high readability (formatting and short names), but low understandability (bad naming)]{Fibonacci program with with high readability (formatting and short names), but low understandability (bad naming)}
%	\label{lst:fibonacci-readable}
%\end{listing}
%
%\begin{listing}[!ht]
%	\begin{minted}[linenos, frame=lines, framesep=2mm]{java}
%		public static int[] generateFibonacciNumbersUntilInput(int n) { int[] fibonacciNumbers = new int[n]; fibonacciNumbers[0] = 0;
%			fibonacciNumbers[1] = 1;
%			for (
%			int i = 2;
%			i < n; i++) {
%				fibonacciNumbers[i] = fibonacciNumbers[i - 1] 
%				+ 
%				fibonacciNumbers[i - 2];
%			}
%			
%			return fibonacciNumbers
%			;
%		}
%	\end{minted}
%	\caption[Fibonacci program with with low readability (formatting and long names), but high understandability (precise naming, low complexity)]{Fibonacci program with with low readability (formatting and long names), but high understandability (precise naming, low complexity)}
%	\label{lst:fibonacci-understandable}
%\end{listing}
%
%
%Thank you for taking the time to understand what code readability is. In the following we ask you to rate the given code snippets by their readability.
	
	\backmatter
	
	\printbibliography
	
\end{document}
